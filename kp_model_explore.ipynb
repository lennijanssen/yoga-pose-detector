{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 16:59:34.111363: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Research\n",
    "\n",
    "MediaPipe has a z-axis output, which apparently estimates the pose in a 2m3 box \n",
    "wherer the origin is the center of the hip. As usual, the point of the algo is \n",
    "to determine the _relataive_ position of the key points, not their absolute\n",
    "position or distance. MP does provide this as `world_coordinates`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple framework:\n",
    "1. Capture the frame from the video feed.\n",
    "2. Process the frame using the pose detection model to find keypoints.\n",
    "3. Draw the keypoints directly onto the frame.\n",
    "4. Display the frame with the drawn keypoints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is some code using MediaPipe to get the key metrics out of the pose estimation.\n",
    "**MODEL COMPLEXITY:** The intended moden (lite, full, heavy) can be adjusted my changing the `model_complexity` kwarg when initializing the `pose` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1707809089.052878       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "2024-02-13 16:24:49.472 python[50146:261858] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "DEPRICATED, USING OLD VERSION WITHOUT VISIBILITY\n",
    "'''\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Pose.\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5,\n",
    "                    min_tracking_confidence=0.5, model_complexity=2)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# Start capturing video from the webcam.\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB.\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame with MediaPipe Pose.\n",
    "    results = pose.process(frame_rgb)\n",
    "\n",
    "    # Draw the pose annotations on the frame.\n",
    "    mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # Display the frame.\n",
    "    cv2.imshow('Mediapipe Feed', frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and destroy all OpenCV windows.\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another implementation example, with some readouts. This is also mediapipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_Sh:\n",
      "x: 0.29915327\n",
      "y: 0.7627407\n",
      "z: -0.4048484\n",
      "visibility: 0.9970653\n",
      "\n",
      "L_Sh:\n",
      "x: 0.78462565\n",
      "y: 0.75210494\n",
      "z: -0.31445843\n",
      "visibility: 0.99767625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DEPRICATED, USING OLD VERSION WITHOUT VISIBILITY\n",
    "'''\n",
    "\n",
    "# Initialize stuff from mediapipe\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Recolor image\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            print(f\"R_Sh:\\n{landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]}\")\n",
    "            print(f\"L_Sh:\\n{landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]}\")\n",
    "\n",
    "            # Save values if torso Z values close to zero\n",
    "            if np.max(np.abs([landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].z,\n",
    "                              landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].z,\n",
    "                              landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].z,\n",
    "                              landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].z])) > 0.5 and\\\n",
    "                np.min([landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].visibility,\n",
    "                        landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].visibility,\n",
    "                        landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].visibility,\n",
    "                        landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].visibility]) > 0.9:\n",
    "                print(\"Can See Torso\")\n",
    "\n",
    "                # TODO Write Framework here to save dataframe of correct torso values.\n",
    "                # The Z value threshold in the above lines need tweaking as well.\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Recolor image back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Show detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "                                  mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2),)\n",
    "\n",
    "        cv2.imshow(\"Mediapipe feed\", image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "        clear_output(wait=True)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with asynchronous processing of frames\n",
    "\n",
    "Another method to run Mediapipe, as described here:\n",
    "https://developers.google.com/mediapipe/solutions/vision/pose_landmarker/python#live-stream\n",
    "\n",
    "Results and performance of each models (lite, full, heavy) are summarized here: https://storage.googleapis.com/mediapipe-assets/Model%20Card%20BlazePose%20GHUM%203D.pdf\n",
    "\n",
    "**Issues with asynchronous processing:** Looks like asynchronous readouts are not suitable if we are doing a video frame overlay. Keep synchronous processing such that overlay is still present with the live feed. The main code difference is using a timestamp in ms and also calling `detect_async()` instead of `detect()`.\n",
    "\n",
    "**From the oracle**\n",
    "> If you're running things synchronously, especially for real-time or live feed scenarios with frameworks like MediaPipe, you typically process each frame one at a time in a loop. In such cases, you don't necessarily need to use a callback function. Instead, you can directly process each frame as you capture it, analyze it with the pose detection model, and immediately draw the keypoints or landmarks onto the frame before displaying it. This approach ensures minimal delay between capturing a frame, processing it, and displaying the results, making it suitable for real-time applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO/Ultralytics\n",
    "\n",
    "YOLO v8 has a pose estimation, but does not include many key ponts (17).\n",
    "Also, YOLO is more known for object detection, hence may not be the best for this application.\n",
    "\n",
    "Table this model for now, can return to it if we have reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting poses from still images for NN model\n",
    "\n",
    "Below is code which takes in a directory, and can output a df including all KP and extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1707809106.394315       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DEPRICATED, USING OLD VERSION WITHOUT VISIBILITY\n",
    "'''\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True, model_complexity=1,\n",
    "                    min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Placeholder for collected data\n",
    "data = []\n",
    "\n",
    "# Specify the directory path containing images\n",
    "directory_path = 'new_pose_data/Yoga-82/images/Warrior_I_Pose_or_Virabhadrasana_I_'\n",
    "\n",
    "# Use glob to get all the image file paths\n",
    "image_files = glob.glob(f'{directory_path}/*.jpg')\n",
    "\n",
    "# Process each image\n",
    "for image_file in image_files:\n",
    "    image = cv2.imread(image_file)\n",
    "    # Ensure the image was correctly loaded before proceeding\n",
    "    if image is not None:\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(image_rgb)\n",
    "\n",
    "        # Initialize a row with the image file name\n",
    "        row = [image_file]\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "            # Flatten all landmark data into the row\n",
    "            for landmark in landmarks:\n",
    "                row.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
    "        else:\n",
    "            # If no landmarks are detected, fill in with None or a placeholder value\n",
    "            row.extend([None] * 33 * 4)\n",
    "\n",
    "        # Append the row to the data list\n",
    "        data.append(row)\n",
    "    else:\n",
    "        print(f\"Failed to load image: {image_file}\")\n",
    "\n",
    "# Define column names\n",
    "columns = ['image_file']\n",
    "for idx in range(1, 34):  # MediaPipe Pose has 33 landmarks, starting index at 1 for readability\n",
    "    prefix = f'kp{idx}_'\n",
    "    attributes = ['x', 'y', 'z', 'visibility']\n",
    "    columns += [f'{prefix}{attr}' for attr in attributes]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pose_landmarker_result.pose_landmarks` gives normalized coordinates in normalized coordinates. Midpoint is the point between the hips.\n",
    "`pose_landmarker_result.pose_world_landmarks` gives the coordinates (not normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1707811194.089459       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "I0000 00:00:1707811206.986461       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "I0000 00:00:1707811210.436236       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "I0000 00:00:1707811228.153658       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "I0000 00:00:1707811241.038104       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "I0000 00:00:1707811245.188779       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "I0000 00:00:1707811257.464644       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "I0000 00:00:1707811264.608255       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "I0000 00:00:1707811276.189028       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "I0000 00:00:1707811287.732780       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "I0000 00:00:1707811293.931320       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "I0000 00:00:1707811310.168095       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "I0000 00:00:1707811316.324147       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "I0000 00:00:1707811340.673658       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import glob\n",
    "\n",
    "model_path = \"model_creator/pose_landmarker_full.task\"\n",
    "\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "PoseLandmarker = mp.tasks.vision.PoseLandmarker\n",
    "PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "options = PoseLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.IMAGE)\n",
    "\n",
    "# Placeholder for collected data\n",
    "attrs_to_get = [\"x\", \"y\", \"z\", \"visibility\", \"presence\"]\n",
    "no_kp_files = []\n",
    "fail_to_load_files = []\n",
    "\n",
    "# Specify the directory path containing images\n",
    "pose_dirs = [p for p in glob.glob(\"selected_poses/*\") if os.path.isdir(p)]\n",
    "pose_names = [os.path.basename(d) for d in pose_dirs]\n",
    "\n",
    "df_all = pd.DataFrame({})\n",
    "\n",
    "for pose in pose_names:\n",
    "    data = []\n",
    "    filenames = []  # List to keep track of filenames\n",
    "    directory_path = f'selected_poses/{pose}'\n",
    "\n",
    "    # Use glob to get all the image file paths\n",
    "    image_files = glob.glob(f'{directory_path}/*.jpg')\n",
    "\n",
    "    # Load the input image from an image file.\n",
    "    # mp_image = mp.Image.create_from_file('selected_poses/akarna/Screenshot 2024-02-06 at 12.50.57.png')\n",
    "\n",
    "    with PoseLandmarker.create_from_options(options) as landmarker:\n",
    "        for image_file in image_files:\n",
    "            # detect key points. object is pose_landmarker_result.pose_landmarks\n",
    "            try:\n",
    "                mp_image = mp.Image.create_from_file(image_file)\n",
    "                pose_landmarker_result = landmarker.detect(mp_image)\n",
    "                if len(pose_landmarker_result.pose_landmarks) > 0:\n",
    "                # extract info out of landmarks object\n",
    "                    row = []\n",
    "                    for lmk in attrs_to_get:\n",
    "                        for x in range(len(pose_landmarker_result.pose_landmarks[0])):\n",
    "                            row.append(getattr(pose_landmarker_result.pose_landmarks[0][x], lmk))\n",
    "                    data.append(row)\n",
    "                    filenames.append(image_file)\n",
    "                else:\n",
    "                    no_kp_files.append(image_file)\n",
    "            except:\n",
    "                fail_to_load_files.append(image_file)\n",
    "                pass\n",
    "\n",
    "    columns = []\n",
    "    for x in range(33):\n",
    "        for attr in attrs_to_get:\n",
    "            col = f\"kp{x}_{attr}\"\n",
    "            columns.append(col)\n",
    "\n",
    "    df_pose = pd.DataFrame(data, columns=columns)\n",
    "    df_pose['filename'] = filenames\n",
    "    df_pose.set_index('filename', inplace=True)\n",
    "    df_pose[\"pose\"] = pose\n",
    "\n",
    "    if len(df_all) < 1:\n",
    "        df_all = df_pose.copy()\n",
    "    else:\n",
    "        df_all = pd.concat([df_all, df_pose], axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comfirming that the number of rows is correct.\n",
    "Note that some of the files do not get read, so nums from below code are overestimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(\"pose_landmark_data_2_13.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downdog: 246 items\n",
      "tree: 41 items\n",
      "boat: 387 items\n",
      "akarna: 78 items\n",
      "warrior: 235 items\n",
      "heron: 141 items\n",
      "goddess: 112 items\n",
      "plank: 112 items\n",
      "revolved_triangle: 508 items\n",
      "cobra: 656 items\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# The path to the directory containing the folders\n",
    "directory_path = 'selected_poses/'\n",
    "\n",
    "# List all items in the directory and filter for directories only\n",
    "folders = [item for item in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, item))]\n",
    "\n",
    "# Initialize a dictionary to hold the folder names and their item counts\n",
    "folder_item_counts = {}\n",
    "\n",
    "# Iterate over each folder and count its contents\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(directory_path, folder)\n",
    "    item_count = len(os.listdir(folder_path))\n",
    "    folder_item_counts[folder] = item_count\n",
    "\n",
    "# Print the count of items in each folder\n",
    "for folder, count in folder_item_counts.items():\n",
    "    print(f\"{folder}: {count} items\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_all\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_all' is not defined"
     ]
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
