{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from IPython.display import clear_output\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Research\n",
    "\n",
    "MediaPipe has a z-axis output, which apparently estimates the pose in a 2m3 box \n",
    "wherer the origin is the center of the hip. As usual, the point of the algo is \n",
    "to determine the _relataive_ position of the key points, not their absolute\n",
    "position or distance. MP does provide this as `world_coordinates`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple framework:\n",
    "1. Capture the frame from the video feed.\n",
    "2. Process the frame using the pose detection model to find keypoints.\n",
    "3. Draw the keypoints directly onto the frame.\n",
    "4. Display the frame with the drawn keypoints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is some code using MediaPipe to get the key metrics out of the pose estimation.\n",
    "**MODEL COMPLEXITY:** The intended moden (lite, full, heavy) can be adjusted my changing the `model_complexity` kwarg when initializing the `pose` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model to /Users/homemasaki/.pyenv/versions/3.10.6/envs/yoga-pose-detector/lib/python3.10/site-packages/mediapipe/modules/pose_landmark/pose_landmark_heavy.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1707103413.135410       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Pose.\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5,\n",
    "                    min_tracking_confidence=0.5, model_complexity=2)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Start capturing video from the webcam.\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB.\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame with MediaPipe Pose.\n",
    "    results = pose.process(frame_rgb)\n",
    "\n",
    "    # Draw the pose annotations on the frame.\n",
    "    mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # Display the frame.\n",
    "    cv2.imshow('Mediapipe Feed', frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and destroy all OpenCV windows.\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another implementation example, with some readouts. This is also mediapipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_Sh:\n",
      "x: 0.30530035\n",
      "y: 0.79772305\n",
      "z: -0.3842694\n",
      "visibility: 0.99893445\n",
      "\n",
      "L_Sh:\n",
      "x: 0.7562555\n",
      "y: 0.8035553\n",
      "z: -0.29845184\n",
      "visibility: 0.9988245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize stuff from mediapipe\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Recolor image\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            print(f\"R_Sh:\\n{landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]}\")\n",
    "            print(f\"L_Sh:\\n{landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]}\")\n",
    "\n",
    "            # Save values if torso Z values close to zero\n",
    "            if np.max(np.abs([landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].z,\n",
    "                              landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].z,\n",
    "                              landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].z,\n",
    "                              landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].z])) > 0.5 and\\\n",
    "                np.min([landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].visibility,\n",
    "                        landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].visibility,\n",
    "                        landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].visibility,\n",
    "                        landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].visibility]) > 0.9:\n",
    "                print(\"Can See Torso\")\n",
    "\n",
    "                # TODO Write Framework here to save dataframe of correct torso values.\n",
    "                # The Z value threshold in the above lines need tweaking as well.\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Recolor image back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Show detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "                                  mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2),)\n",
    "\n",
    "        cv2.imshow(\"Mediapipe feed\", image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "        clear_output(wait=True)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with asynchronous processing of frames\n",
    "\n",
    "Another method to run Mediapipe, as described here:\n",
    "https://developers.google.com/mediapipe/solutions/vision/pose_landmarker/python#live-stream\n",
    "\n",
    "Results and performance of each models (lite, full, heavy) are summarized here: https://storage.googleapis.com/mediapipe-assets/Model%20Card%20BlazePose%20GHUM%203D.pdf\n",
    "\n",
    "**Issues with asynchronous processing:** Looks like asynchronous readouts are not suitable if we are doing a video frame overlay. Keep synchronous processing such that overlay is still present with the live feed. The main code difference is using a timestamp in ms and also calling `detect_async()` instead of `detect()`.\n",
    "\n",
    "**From the oracle**\n",
    "> If you're running things synchronously, especially for real-time or live feed scenarios with frameworks like MediaPipe, you typically process each frame one at a time in a loop. In such cases, you don't necessarily need to use a callback function. Instead, you can directly process each frame as you capture it, analyze it with the pose detection model, and immediately draw the keypoints or landmarks onto the frame before displaying it. This approach ensures minimal delay between capturing a frame, processing it, and displaying the results, making it suitable for real-time applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The vision task is in live stream mode, a user-defined result callback must be provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Grab frame using OpenCV\u001b[39;00m\n\u001b[1;32m     33\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPoseLandmarker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_from_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m landmarker:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m         \u001b[38;5;66;03m# Grab frame using OpenCV. Assume frame is grabbed.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m         _, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/fitme-env/lib/python3.10/site-packages/mediapipe/tasks/python/vision/pose_landmarker.py:319\u001b[0m, in \u001b[0;36mPoseLandmarker.create_from_options\u001b[0;34m(cls, options)\u001b[0m\n\u001b[1;32m    306\u001b[0m   output_streams\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    307\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([_SEGMENTATION_MASK_TAG, _SEGMENTATION_MASK_STREAM_NAME])\n\u001b[1;32m    308\u001b[0m   )\n\u001b[1;32m    310\u001b[0m task_info \u001b[38;5;241m=\u001b[39m _TaskInfo(\n\u001b[1;32m    311\u001b[0m     task_graph\u001b[38;5;241m=\u001b[39m_TASK_GRAPH_NAME,\n\u001b[1;32m    312\u001b[0m     input_streams\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    317\u001b[0m     task_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    318\u001b[0m )\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_graph_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_flow_limiting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mode\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_RunningMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLIVE_STREAM\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpackets_callback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_callback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/fitme-env/lib/python3.10/site-packages/mediapipe/tasks/python/vision/core/base_vision_task_api.py:61\u001b[0m, in \u001b[0;36mBaseVisionTaskApi.__init__\u001b[0;34m(self, graph_config, running_mode, packet_callback)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m running_mode \u001b[38;5;241m==\u001b[39m _RunningMode\u001b[38;5;241m.\u001b[39mLIVE_STREAM:\n\u001b[1;32m     60\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m packet_callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe vision task is in live stream mode, a user-defined result \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback must be provided.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m packet_callback:\n\u001b[1;32m     66\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     67\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe vision task is in image or video mode, a user-defined result \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     68\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback should not be provided.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     69\u001b[0m   )\n",
      "\u001b[0;31mValueError\u001b[0m: The vision task is in live stream mode, a user-defined result callback must be provided."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Belwo is a playaround with asynchronous processing, which is a bit more complex\n",
    "than running a simple live feed. This method is better if we do not have any\n",
    "overlay on the video, but ignore this part since it is not quite critical\n",
    "to run thing asynchronously. Code is kept in notebook for reference.\n",
    "\"\"\"\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import time\n",
    "\n",
    "# Load model\n",
    "model_path = \"/Users/homemasaki/code/projects/fit_me/models/pose_landmarker_full.task\"\n",
    "\n",
    "# Drawing utility\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Create the task\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "PoseLandmarker = mp.tasks.vision.PoseLandmarker\n",
    "PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "# Create a pose landmarker instance with the live stream mode:\n",
    "\n",
    "options = PoseLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.LIVE_STREAM)\n",
    "\n",
    "# Grab frame using OpenCV\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with PoseLandmarker.create_from_options(options) as landmarker:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Grab frame using OpenCV. Assume frame is grabbed.\n",
    "        _, frame = cap.read()\n",
    "\n",
    "        # Convert frame to MediaPipe Image object\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame.tobytes(),\n",
    "                            width=frame.shape[1], height=frame.shape[0])\n",
    "        landmarker.detect(mp_image)\n",
    "\n",
    "        #cv2.imshow(\"Mediapipe feed\", frame)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO/Ultralytics\n",
    "\n",
    "YOLO v8 has a pose estimation, but does not include many key ponts (17).\n",
    "Also, YOLO is more known for object detection, hence may not be the best for this application.\n",
    "\n",
    "Table this model for now, can return to it if we have reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting poses from still images for NN model\n",
    "\n",
    "Below is code which takes in a directory, and can output a df including all KP and extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1707106777.192229       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "[ WARN:0@8995.056] global loadsave.cpp:248 findDecoder imread_('path/to/image1.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) /Users/runner/work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_file \u001b[38;5;129;01min\u001b[39;00m image_files:\n\u001b[1;32m     18\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_file)\n\u001b[0;32m---> 19\u001b[0m     image_rgb \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     results \u001b[38;5;241m=\u001b[39m pose\u001b[38;5;241m.\u001b[39mprocess(image_rgb)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Initialize a row with the image file name\u001b[39;00m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.9.0) /Users/runner/work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True, model_complexity=1,\n",
    "                    min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Placeholder for collected data\n",
    "data = []\n",
    "\n",
    "# File paths here\n",
    "image_files = ['path/to/image1.jpg', ...]\n",
    "\n",
    "# Process each image\n",
    "for image_file in image_files:\n",
    "    image = cv2.imread(image_file)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "\n",
    "    # Initialize a row with the image file name\n",
    "    row = [image_file]\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "        # Flatten all landmark data into the row\n",
    "        for landmark in landmarks:\n",
    "            row.extend([landmark.x, landmark.y, landmark.z, landmark.visibility, landmark.presence])\n",
    "    else:\n",
    "        # If no landmarks are detected, fill in with None or a placeholder value\n",
    "        row.extend([None] * 33 * 5)\n",
    "\n",
    "    # Append the row to the data list\n",
    "    data.append(row)\n",
    "\n",
    "# Define column names\n",
    "columns = ['image_file']\n",
    "for idx in range(1, 34):  # MediaPipe Pose has 33 landmarks, starting index at 1 for readability\n",
    "    prefix = f'kp{idx}_'\n",
    "    attributes = ['x', 'y', 'z', 'visibility', 'presence']\n",
    "    columns += [f'{prefix}{attr}' for attr in attributes]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Optionally, save DataFrame to CSV\n",
    "df.to_csv('pose_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
