{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 11:26:14.191855: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Research\n",
    "\n",
    "MediaPipe has a z-axis output, which apparently estimates the pose in a 2m3 box \n",
    "wherer the origin is the center of the hip. As usual, the point of the algo is \n",
    "to determine the _relataive_ position of the key points, not their absolute\n",
    "position or distance. MP does provide this as `world_coordinates`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple framework:\n",
    "1. Capture the frame from the video feed.\n",
    "2. Process the frame using the pose detection model to find keypoints.\n",
    "3. Draw the keypoints directly onto the frame.\n",
    "4. Display the frame with the drawn keypoints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is some code using MediaPipe to get the key metrics out of the pose estimation.\n",
    "**MODEL COMPLEXITY:** The intended moden (lite, full, heavy) can be adjusted my changing the `model_complexity` kwarg when initializing the `pose` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1707185922.848916       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "2024-02-06 11:18:43.418 python[30127:181416] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "'''\n",
    "DEPRICATED, USING OLD VERSION WITHOUT VISIBILITY\n",
    "'''\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Pose.\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5,\n",
    "                    min_tracking_confidence=0.5, model_complexity=2)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Start capturing video from the webcam.\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB.\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame with MediaPipe Pose.\n",
    "    results = pose.process(frame_rgb)\n",
    "\n",
    "    # Draw the pose annotations on the frame.\n",
    "    mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "    # Display the frame.\n",
    "    cv2.imshow('Mediapipe Feed', frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and destroy all OpenCV windows.\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another implementation example, with some readouts. This is also mediapipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_Sh:\n",
      "x: 0.29915327\n",
      "y: 0.7627407\n",
      "z: -0.4048484\n",
      "visibility: 0.9970653\n",
      "\n",
      "L_Sh:\n",
      "x: 0.78462565\n",
      "y: 0.75210494\n",
      "z: -0.31445843\n",
      "visibility: 0.99767625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DEPRICATED, USING OLD VERSION WITHOUT VISIBILITY\n",
    "'''\n",
    "\n",
    "# Initialize stuff from mediapipe\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Recolor image\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            print(f\"R_Sh:\\n{landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]}\")\n",
    "            print(f\"L_Sh:\\n{landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]}\")\n",
    "\n",
    "            # Save values if torso Z values close to zero\n",
    "            if np.max(np.abs([landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].z,\n",
    "                              landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].z,\n",
    "                              landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].z,\n",
    "                              landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].z])) > 0.5 and\\\n",
    "                np.min([landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].visibility,\n",
    "                        landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].visibility,\n",
    "                        landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].visibility,\n",
    "                        landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].visibility]) > 0.9:\n",
    "                print(\"Can See Torso\")\n",
    "\n",
    "                # TODO Write Framework here to save dataframe of correct torso values.\n",
    "                # The Z value threshold in the above lines need tweaking as well.\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Recolor image back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Show detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "                                  mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2),)\n",
    "\n",
    "        cv2.imshow(\"Mediapipe feed\", image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "        clear_output(wait=True)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with asynchronous processing of frames\n",
    "\n",
    "Another method to run Mediapipe, as described here:\n",
    "https://developers.google.com/mediapipe/solutions/vision/pose_landmarker/python#live-stream\n",
    "\n",
    "Results and performance of each models (lite, full, heavy) are summarized here: https://storage.googleapis.com/mediapipe-assets/Model%20Card%20BlazePose%20GHUM%203D.pdf\n",
    "\n",
    "**Issues with asynchronous processing:** Looks like asynchronous readouts are not suitable if we are doing a video frame overlay. Keep synchronous processing such that overlay is still present with the live feed. The main code difference is using a timestamp in ms and also calling `detect_async()` instead of `detect()`.\n",
    "\n",
    "**From the oracle**\n",
    "> If you're running things synchronously, especially for real-time or live feed scenarios with frameworks like MediaPipe, you typically process each frame one at a time in a loop. In such cases, you don't necessarily need to use a callback function. Instead, you can directly process each frame as you capture it, analyze it with the pose detection model, and immediately draw the keypoints or landmarks onto the frame before displaying it. This approach ensures minimal delay between capturing a frame, processing it, and displaying the results, making it suitable for real-time applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The vision task is in live stream mode, a user-defined result callback must be provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Grab frame using OpenCV\u001b[39;00m\n\u001b[1;32m     33\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPoseLandmarker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_from_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m landmarker:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m         \u001b[38;5;66;03m# Grab frame using OpenCV. Assume frame is grabbed.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m         _, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/yoga-pose-detector/lib/python3.10/site-packages/mediapipe/tasks/python/vision/pose_landmarker.py:319\u001b[0m, in \u001b[0;36mPoseLandmarker.create_from_options\u001b[0;34m(cls, options)\u001b[0m\n\u001b[1;32m    306\u001b[0m   output_streams\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    307\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([_SEGMENTATION_MASK_TAG, _SEGMENTATION_MASK_STREAM_NAME])\n\u001b[1;32m    308\u001b[0m   )\n\u001b[1;32m    310\u001b[0m task_info \u001b[38;5;241m=\u001b[39m _TaskInfo(\n\u001b[1;32m    311\u001b[0m     task_graph\u001b[38;5;241m=\u001b[39m_TASK_GRAPH_NAME,\n\u001b[1;32m    312\u001b[0m     input_streams\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    317\u001b[0m     task_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    318\u001b[0m )\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_graph_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_flow_limiting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mode\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_RunningMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLIVE_STREAM\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpackets_callback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_callback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/yoga-pose-detector/lib/python3.10/site-packages/mediapipe/tasks/python/vision/core/base_vision_task_api.py:61\u001b[0m, in \u001b[0;36mBaseVisionTaskApi.__init__\u001b[0;34m(self, graph_config, running_mode, packet_callback)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m running_mode \u001b[38;5;241m==\u001b[39m _RunningMode\u001b[38;5;241m.\u001b[39mLIVE_STREAM:\n\u001b[1;32m     60\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m packet_callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe vision task is in live stream mode, a user-defined result \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback must be provided.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m packet_callback:\n\u001b[1;32m     66\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     67\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe vision task is in image or video mode, a user-defined result \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     68\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback should not be provided.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     69\u001b[0m   )\n",
      "\u001b[0;31mValueError\u001b[0m: The vision task is in live stream mode, a user-defined result callback must be provided."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Belwo is a playaround with asynchronous processing, which is a bit more complex\n",
    "than running a simple live feed. This method is better if we do not have any\n",
    "overlay on the video, but ignore this part since it is not quite critical\n",
    "to run thing asynchronously. Code is kept in notebook for reference.\n",
    "\"\"\"\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import time\n",
    "\n",
    "# Load model\n",
    "model_path = \"/Users/homemasaki/code/projects/fit_me/models/pose_landmarker_full.task\"\n",
    "\n",
    "# Drawing utility\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Create the task\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "PoseLandmarker = mp.tasks.vision.PoseLandmarker\n",
    "PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "# Create a pose landmarker instance with the live stream mode:\n",
    "\n",
    "options = PoseLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.LIVE_STREAM)\n",
    "\n",
    "# Grab frame using OpenCV\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with PoseLandmarker.create_from_options(options) as landmarker:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Grab frame using OpenCV. Assume frame is grabbed.\n",
    "        _, frame = cap.read()\n",
    "\n",
    "        # Convert frame to MediaPipe Image object\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame.tobytes(),\n",
    "                            width=frame.shape[1], height=frame.shape[0])\n",
    "        landmarker.detect(mp_image)\n",
    "\n",
    "        #cv2.imshow(\"Mediapipe feed\", frame)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO/Ultralytics\n",
    "\n",
    "YOLO v8 has a pose estimation, but does not include many key ponts (17).\n",
    "Also, YOLO is more known for object detection, hence may not be the best for this application.\n",
    "\n",
    "Table this model for now, can return to it if we have reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting poses from still images for NN model\n",
    "\n",
    "Below is code which takes in a directory, and can output a df including all KP and extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1707193516.069692       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load image: new_pose_data/Yoga-82/images/Warrior_I_Pose_or_Virabhadrasana_I_/Warrior_I_Pose_or_Virabhadrasana_I__1_0_493.jpg\n",
      "Failed to load image: new_pose_data/Yoga-82/images/Warrior_I_Pose_or_Virabhadrasana_I_/Warrior_I_Pose_or_Virabhadrasana_I__2_499.jpg\n",
      "Failed to load image: new_pose_data/Yoga-82/images/Warrior_I_Pose_or_Virabhadrasana_I_/Warrior_I_Pose_or_Virabhadrasana_I__2_59.jpg\n",
      "Failed to load image: new_pose_data/Yoga-82/images/Warrior_I_Pose_or_Virabhadrasana_I_/Warrior_I_Pose_or_Virabhadrasana_I__1_0_149.jpg\n",
      "Failed to load image: new_pose_data/Yoga-82/images/Warrior_I_Pose_or_Virabhadrasana_I_/Warrior_I_Pose_or_Virabhadrasana_I__1_0_639.jpg\n",
      "Failed to load image: new_pose_data/Yoga-82/images/Warrior_I_Pose_or_Virabhadrasana_I_/Warrior_I_Pose_or_Virabhadrasana_I__1_624.jpg\n",
      "Failed to load image: new_pose_data/Yoga-82/images/Warrior_I_Pose_or_Virabhadrasana_I_/Warrior_I_Pose_or_Virabhadrasana_I__1_330.jpg\n",
      "Failed to load image: new_pose_data/Yoga-82/images/Warrior_I_Pose_or_Virabhadrasana_I_/Warrior_I_Pose_or_Virabhadrasana_I__1_1.jpg\n",
      "Failed to load image: new_pose_data/Yoga-82/images/Warrior_I_Pose_or_Virabhadrasana_I_/Warrior_I_Pose_or_Virabhadrasana_I__2_18.jpg\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DEPRICATED, USING OLD VERSION WITHOUT VISIBILITY\n",
    "'''\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True, model_complexity=1,\n",
    "                    min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Placeholder for collected data\n",
    "data = []\n",
    "\n",
    "# Specify the directory path containing images\n",
    "directory_path = 'new_pose_data/Yoga-82/images/Warrior_I_Pose_or_Virabhadrasana_I_'\n",
    "\n",
    "# Use glob to get all the image file paths\n",
    "image_files = glob.glob(f'{directory_path}/*.jpg')\n",
    "\n",
    "# Process each image\n",
    "for image_file in image_files:\n",
    "    image = cv2.imread(image_file)\n",
    "    # Ensure the image was correctly loaded before proceeding\n",
    "    if image is not None:\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(image_rgb)\n",
    "\n",
    "        # Initialize a row with the image file name\n",
    "        row = [image_file]\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "            # Flatten all landmark data into the row\n",
    "            for landmark in landmarks:\n",
    "                row.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
    "        else:\n",
    "            # If no landmarks are detected, fill in with None or a placeholder value\n",
    "            row.extend([None] * 33 * 4)\n",
    "\n",
    "        # Append the row to the data list\n",
    "        data.append(row)\n",
    "    else:\n",
    "        print(f\"Failed to load image: {image_file}\")\n",
    "\n",
    "# Define column names\n",
    "columns = ['image_file']\n",
    "for idx in range(1, 34):  # MediaPipe Pose has 33 landmarks, starting index at 1 for readability\n",
    "    prefix = f'kp{idx}_'\n",
    "    attributes = ['x', 'y', 'z', 'visibility']\n",
    "    columns += [f'{prefix}{attr}' for attr in attributes]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pose_landmarker_result.pose_landmarks` gives normalized coordinates in normalized coordinates. Midpoint is the point between the hips.\n",
    "`pose_landmarker_result.pose_world_landmarks` gives the coordinates (not normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1707271712.322673       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "I0000 00:00:1707271727.450679       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "I0000 00:00:1707271729.878667       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "I0000 00:00:1707271748.834669       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "I0000 00:00:1707271753.315147       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "I0000 00:00:1707271766.434786       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "I0000 00:00:1707271774.053969       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "I0000 00:00:1707271781.302527       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "I0000 00:00:1707271788.337440       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n",
      "I0000 00:00:1707271814.815212       1 gl_context.cc:344] GL version: 2.1 (2.1 INTEL-22.1.29), renderer: Intel(R) Iris(TM) Plus Graphics 655\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import glob\n",
    "\n",
    "model_path = \"model_creator/pose_landmarker_full.task\"\n",
    "\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "PoseLandmarker = mp.tasks.vision.PoseLandmarker\n",
    "PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "options = PoseLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.IMAGE)\n",
    "\n",
    "# Placeholder for collected data\n",
    "attrs_to_get = [\"x\", \"y\", \"z\", \"visibility\", \"presence\"]\n",
    "no_kp_files = []\n",
    "fail_to_load_files = []\n",
    "\n",
    "# Specify the directory path containing images\n",
    "pose_dirs = [p for p in glob.glob(\"selected_poses/*\") if os.path.isdir(p)]\n",
    "pose_names = [os.path.basename(d) for d in pose_dirs]\n",
    "\n",
    "df_all = pd.DataFrame({})\n",
    "\n",
    "for pose in pose_names:\n",
    "    data = []\n",
    "    directory_path = f'selected_poses/{pose}'\n",
    "\n",
    "    # Use glob to get all the image file paths\n",
    "    image_files = glob.glob(f'{directory_path}/*.jpg')\n",
    "\n",
    "    # Load the input image from an image file.\n",
    "    # mp_image = mp.Image.create_from_file('selected_poses/akarna/Screenshot 2024-02-06 at 12.50.57.png')\n",
    "\n",
    "    with PoseLandmarker.create_from_options(options) as landmarker:\n",
    "        for image_file in image_files:\n",
    "            # detect key points. object is pose_landmarker_result.pose_landmarks\n",
    "            try:\n",
    "                mp_image = mp.Image.create_from_file(image_file)\n",
    "                pose_landmarker_result = landmarker.detect(mp_image)\n",
    "                if len(pose_landmarker_result.pose_landmarks) > 0:\n",
    "                # extract info out of landmarks object\n",
    "                    row = []\n",
    "                    for lmk in attrs_to_get:\n",
    "                        for x in range(len(pose_landmarker_result.pose_landmarks[0])):\n",
    "                            row.append(getattr(pose_landmarker_result.pose_landmarks[0][x], lmk))\n",
    "                    data.append(row)\n",
    "                else:\n",
    "                    no_kp_files.append(image_file)\n",
    "            except:\n",
    "                fail_to_load_files.append(image_file)\n",
    "                pass\n",
    "\n",
    "    columns = []\n",
    "    for x in range(33):\n",
    "        for attr in attrs_to_get:\n",
    "            col = f\"kp{x}_{attr}\"\n",
    "            columns.append(col)\n",
    "\n",
    "    df_pose = pd.DataFrame(data, columns=columns)\n",
    "    df_pose[\"pose\"] = pose\n",
    "\n",
    "    if len(df_all) < 1:\n",
    "        df_all = df_pose.copy()\n",
    "    else:\n",
    "        df_all = pd.concat([df_all, df_pose], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pose\n",
       "cobra                554\n",
       "revolved_triangle    445\n",
       "boat                 321\n",
       "downdog              219\n",
       "warrior              219\n",
       "heron                124\n",
       "plank                105\n",
       "goddess               99\n",
       "akarna                70\n",
       "tree                  34\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all[\"pose\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comfirming that the number of rows is correct.\n",
    "Note that some of the files do not get read, so nums from below code are overestimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(\"pose_landmark_data_10.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downdog: 246 items\n",
      "tree: 41 items\n",
      "boat: 387 items\n",
      "akarna: 78 items\n",
      "warrior: 235 items\n",
      "heron: 141 items\n",
      "goddess: 112 items\n",
      "plank: 112 items\n",
      "revolved_triangle: 508 items\n",
      "cobra: 656 items\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# The path to the directory containing the folders\n",
    "directory_path = 'selected_poses/'\n",
    "\n",
    "# List all items in the directory and filter for directories only\n",
    "folders = [item for item in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, item))]\n",
    "\n",
    "# Initialize a dictionary to hold the folder names and their item counts\n",
    "folder_item_counts = {}\n",
    "\n",
    "# Iterate over each folder and count its contents\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(directory_path, folder)\n",
    "    item_count = len(os.listdir(folder_path))\n",
    "    folder_item_counts[folder] = item_count\n",
    "\n",
    "# Print the count of items in each folder\n",
    "for folder, count in folder_item_counts.items():\n",
    "    print(f\"{folder}: {count} items\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
