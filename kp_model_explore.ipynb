{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from IPython.display import clear_output\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Research\n",
    "\n",
    "MediaPipe has a z-axis output, which apparently estimates the pose in a 2m3 box \n",
    "wherer the origin is the center of the hip. As usual, the point of the algo is \n",
    "to determine the _relataive_ position of the key points, not their absolute\n",
    "position or distance.\n",
    "\n",
    "Nevertheless, some code needs to be written to estimate the proportions of each\n",
    "subject. The question is: Assume that the person is facing orthogonal to the \n",
    "camera when their body makes the largest \"frame\". This is the most accurate 2D\n",
    "depiction of their body. How do you determine when the subhect is orthogonal\n",
    "to the camera?\n",
    "\n",
    "The easiest approach is to figure out when the subject has the same z-axis\n",
    "readout on both shoulders. This would at least mean that the torso is close to being orthogonal to the \n",
    "camera. If the data collects readouts that are within some z-axis threshold,\n",
    "this could be used to re-create the model with the target ratios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some thoughts on UI\n",
    "- Would be good if it had a text readout of what your body is \"kinda like\"\n",
    "so it will give the user some assurance that the thing is working\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is some code using MediaPipe to get the key metrics out of the pose estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stuff from mediapipe\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_Sh:\n",
      "x: 0.30530035\n",
      "y: 0.79772305\n",
      "z: -0.3842694\n",
      "visibility: 0.99893445\n",
      "\n",
      "L_Sh:\n",
      "x: 0.7562555\n",
      "y: 0.8035553\n",
      "z: -0.29845184\n",
      "visibility: 0.9988245\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Recolor image\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            print(f\"R_Sh:\\n{landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]}\")\n",
    "            print(f\"L_Sh:\\n{landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]}\")\n",
    "\n",
    "            # Save values if torso Z values close to zero\n",
    "            if np.max(np.abs([landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].z,\n",
    "                              landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].z,\n",
    "                              landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].z,\n",
    "                              landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].z])) > 0.5 and\\\n",
    "                np.min([landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].visibility,\n",
    "                        landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].visibility,\n",
    "                        landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].visibility,\n",
    "                        landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].visibility]) > 0.9:\n",
    "                print(\"Can See Torso\")\n",
    "\n",
    "                # TODO Write Framework here to save dataframe of correct torso values.\n",
    "                # The Z value threshold in the above lines need tweaking as well.\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Recolor image back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Show detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "                                  mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2),)\n",
    "\n",
    "        cv2.imshow(\"Mediapipe feed\", image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "        clear_output(wait=True)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method to run Mediapipe, as described here:\n",
    "https://developers.google.com/mediapipe/solutions/vision/pose_landmarker/python#live-stream\n",
    "\n",
    "Results and performance of each models (lite, full, heavy) are summarized here: https://storage.googleapis.com/mediapipe-assets/Model%20Card%20BlazePose%20GHUM%203D.pdf\n",
    "\n",
    "**Issues with asynchronous processing:** Looks like asynchronous readouts are not suitable if we are doing a video frame overlay. Keep synchronous processing such that overlay is still present with the live feed. The main code difference is using a timestamp in ms and also calling `detect_async()` instead of `detect()`.\n",
    "\n",
    "**From the oracle**\n",
    "> If you're running things synchronously, especially for real-time or live feed scenarios with frameworks like MediaPipe, you typically process each frame one at a time in a loop. In such cases, you don't necessarily need to use a callback function. Instead, you can directly process each frame as you capture it, analyze it with the pose detection model, and immediately draw the keypoints or landmarks onto the frame before displaying it. This approach ensures minimal delay between capturing a frame, processing it, and displaying the results, making it suitable for real-time applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The vision task is in live stream mode, a user-defined result callback must be provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Grab frame using OpenCV\u001b[39;00m\n\u001b[1;32m     33\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPoseLandmarker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_from_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m landmarker:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m         \u001b[38;5;66;03m# Grab frame using OpenCV. Assume frame is grabbed.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m         _, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/fitme-env/lib/python3.10/site-packages/mediapipe/tasks/python/vision/pose_landmarker.py:319\u001b[0m, in \u001b[0;36mPoseLandmarker.create_from_options\u001b[0;34m(cls, options)\u001b[0m\n\u001b[1;32m    306\u001b[0m   output_streams\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    307\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([_SEGMENTATION_MASK_TAG, _SEGMENTATION_MASK_STREAM_NAME])\n\u001b[1;32m    308\u001b[0m   )\n\u001b[1;32m    310\u001b[0m task_info \u001b[38;5;241m=\u001b[39m _TaskInfo(\n\u001b[1;32m    311\u001b[0m     task_graph\u001b[38;5;241m=\u001b[39m_TASK_GRAPH_NAME,\n\u001b[1;32m    312\u001b[0m     input_streams\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    317\u001b[0m     task_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    318\u001b[0m )\n\u001b[0;32m--> 319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_graph_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_flow_limiting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mode\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_RunningMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLIVE_STREAM\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpackets_callback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_callback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/fitme-env/lib/python3.10/site-packages/mediapipe/tasks/python/vision/core/base_vision_task_api.py:61\u001b[0m, in \u001b[0;36mBaseVisionTaskApi.__init__\u001b[0;34m(self, graph_config, running_mode, packet_callback)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m running_mode \u001b[38;5;241m==\u001b[39m _RunningMode\u001b[38;5;241m.\u001b[39mLIVE_STREAM:\n\u001b[1;32m     60\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m packet_callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe vision task is in live stream mode, a user-defined result \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback must be provided.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m packet_callback:\n\u001b[1;32m     66\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     67\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe vision task is in image or video mode, a user-defined result \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     68\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback should not be provided.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     69\u001b[0m   )\n",
      "\u001b[0;31mValueError\u001b[0m: The vision task is in live stream mode, a user-defined result callback must be provided."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Belwo is a playaround with asynchronous processing, which is a bit more complex\n",
    "than running a simple live feed. This method is better if we do not have any\n",
    "overlay on the video, but ignore this part since it is not quite critical\n",
    "to run thing asynchronously. Code is kept in notebook for reference.\n",
    "\"\"\"\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import time\n",
    "\n",
    "# Load model\n",
    "model_path = \"/Users/homemasaki/code/projects/fit_me/models/pose_landmarker_full.task\"\n",
    "\n",
    "# Drawing utility\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Create the task\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "PoseLandmarker = mp.tasks.vision.PoseLandmarker\n",
    "PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "# Create a pose landmarker instance with the live stream mode:\n",
    "\n",
    "options = PoseLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.LIVE_STREAM)\n",
    "\n",
    "# Grab frame using OpenCV\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with PoseLandmarker.create_from_options(options) as landmarker:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Grab frame using OpenCV. Assume frame is grabbed.\n",
    "        _, frame = cap.read()\n",
    "\n",
    "        # Convert frame to MediaPipe Image object\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame.tobytes(),\n",
    "                            width=frame.shape[1], height=frame.shape[0])\n",
    "        landmarker.detect(mp_image)\n",
    "\n",
    "        #cv2.imshow(\"Mediapipe feed\", frame)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
